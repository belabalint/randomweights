{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(784, 128), nn.ReLU(), nn.Linear(128, 128), nn.ReLU(), nn.Linear(128, 10), nn.LogSoftmax(dim=1))\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.003)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.9038798787446418\n",
      "Training loss: 0.8089508007902072\n",
      "Training loss: 0.5033564670348981\n",
      "Training loss: 0.4190176468827069\n",
      "Training loss: 0.37895376155816163\n",
      "Training loss: 0.3546028422918528\n",
      "Training loss: 0.3380051120273721\n",
      "Training loss: 0.32416532577069074\n",
      "Training loss: 0.31299500636009775\n",
      "Training loss: 0.30333574540388863\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        images = images.view(images.shape[0], -1)\n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss/len(trainloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.4473)\n",
      "tensor(1.5396)\n",
      "tensor(1.0072)\n",
      "tensor(1.6109)\n",
      "tensor(1.5235)\n",
      "tensor(1.4786)\n",
      "tensor(1.4338)\n",
      "tensor(1.7368)\n",
      "tensor(1.3385)\n",
      "tensor(1.1031)\n",
      "tensor(1.4730)\n",
      "tensor(2.1605)\n",
      "tensor(0.6014)\n",
      "tensor(1.6672)\n",
      "tensor(1.5381)\n",
      "tensor(1.6750)\n",
      "tensor(2.1693)\n",
      "tensor(0.7145)\n",
      "tensor(1.5834)\n",
      "tensor(2.0455)\n",
      "tensor(1.4113)\n",
      "tensor(1.7779)\n",
      "tensor(1.6848)\n",
      "tensor(1.2912)\n",
      "tensor(1.4487)\n",
      "tensor(1.6174)\n",
      "tensor(1.7020)\n",
      "tensor(1.4471)\n",
      "tensor(1.7368)\n",
      "tensor(1.5161)\n",
      "tensor(1.3387)\n",
      "tensor(1.8869)\n",
      "tensor(1.1843)\n",
      "tensor(0.9030)\n",
      "tensor(1.2797)\n",
      "tensor(1.5973)\n",
      "tensor(1.5529)\n",
      "tensor(2.2353)\n",
      "tensor(1.3589)\n",
      "tensor(1.5633)\n",
      "tensor(1.7707)\n",
      "tensor(1.7943)\n",
      "tensor(1.5351)\n",
      "tensor(1.1112)\n",
      "tensor(1.1003)\n",
      "tensor(1.6638)\n",
      "tensor(1.1002)\n",
      "tensor(1.3346)\n",
      "tensor(1.8078)\n",
      "tensor(0.8811)\n",
      "tensor(1.6416)\n",
      "tensor(1.8552)\n",
      "tensor(1.2926)\n",
      "tensor(1.1474)\n",
      "tensor(1.2047)\n",
      "tensor(2.7472)\n",
      "tensor(1.1089)\n",
      "tensor(1.3701)\n",
      "tensor(1.7128)\n",
      "tensor(0.9178)\n",
      "tensor(0.2153)\n",
      "tensor(1.3397)\n",
      "tensor(1.9820)\n",
      "tensor(2.4625)\n",
      "tensor(1.8859)\n",
      "tensor(1.9754)\n",
      "tensor(1.6892)\n",
      "tensor(0.8091)\n",
      "tensor(1.7688)\n",
      "tensor(1.5364)\n",
      "tensor(1.9923)\n",
      "tensor(1.0344)\n",
      "tensor(2.1392)\n",
      "tensor(1.9967)\n",
      "tensor(1.4594)\n",
      "tensor(1.4883)\n",
      "tensor(1.6992)\n",
      "tensor(1.4199)\n",
      "tensor(1.8444)\n",
      "tensor(1.8585)\n",
      "tensor(1.3441)\n",
      "tensor(1.5037)\n",
      "tensor(1.5323)\n",
      "tensor(1.8150)\n",
      "tensor(1.4638)\n",
      "tensor(1.6660)\n",
      "tensor(1.9675)\n",
      "tensor(1.0255)\n",
      "tensor(1.7694)\n",
      "tensor(1.7132)\n",
      "tensor(1.2694)\n",
      "tensor(1.3990)\n",
      "tensor(1.2809)\n",
      "tensor(1.0195)\n",
      "tensor(2.2410)\n",
      "tensor(1.8428)\n",
      "tensor(1.3293)\n",
      "tensor(1.2280)\n",
      "tensor(2.0816)\n",
      "tensor(1.9041)\n",
      "tensor(1.4613)\n",
      "tensor(1.4966)\n",
      "tensor(1.8056)\n",
      "tensor(1.6525)\n",
      "tensor(2.2382)\n",
      "tensor(1.4245)\n",
      "tensor(1.0839)\n",
      "tensor(1.3639)\n",
      "tensor(1.0737)\n",
      "tensor(1.6190)\n",
      "tensor(1.1413)\n",
      "tensor(2.2553)\n",
      "tensor(1.9620)\n",
      "tensor(1.4411)\n",
      "tensor(2.1888)\n",
      "tensor(1.7470)\n",
      "tensor(1.0699)\n",
      "tensor(1.7624)\n",
      "tensor(1.3202)\n",
      "tensor(2.5691)\n",
      "tensor(2.1640)\n",
      "tensor(1.5458)\n",
      "tensor(2.1597)\n",
      "tensor(1.2383)\n",
      "tensor(2.1816)\n",
      "tensor(2.2643)\n",
      "tensor(1.4864)\n",
      "tensor(1.7448)\n",
      "tensor(1.5503)\n",
      "tensor(1.7157)\n",
      "tensor(1.2222)\n",
      "tensor(1.8763)\n",
      "tensor(1.3002)\n",
      "tensor(1.7240)\n",
      "tensor(2.1095)\n",
      "tensor(0.3613)\n",
      "tensor(1.5126)\n",
      "tensor(1.4682)\n",
      "tensor(1.2414)\n",
      "tensor(2.3741)\n",
      "tensor(1.4659)\n",
      "tensor(2.1870)\n",
      "tensor(1.6565)\n",
      "tensor(1.6158)\n",
      "tensor(1.8181)\n",
      "tensor(1.1979)\n",
      "tensor(1.1598)\n",
      "tensor(1.1940)\n",
      "tensor(1.6105)\n",
      "tensor(1.3767)\n",
      "tensor(0.9445)\n",
      "tensor(1.4764)\n",
      "tensor(1.6621)\n",
      "tensor(1.4148)\n",
      "tensor(0.2142)\n",
      "tensor(1.6955)\n",
      "tensor(1.0455)\n",
      "tensor(1.4279)\n",
      "tensor(1.1938)\n",
      "tensor(1.2380)\n",
      "tensor(1.5918)\n",
      "tensor(1.8389)\n",
      "tensor(1.4395)\n",
      "tensor(1.1822)\n",
      "tensor(2.0239)\n",
      "tensor(0.9871)\n",
      "tensor(2.1221)\n",
      "tensor(1.7375)\n",
      "tensor(1.6222)\n",
      "tensor(1.7880)\n",
      "tensor(1.1707)\n",
      "tensor(2.3430)\n",
      "tensor(1.3074)\n",
      "tensor(2.7883)\n",
      "tensor(1.5057)\n",
      "tensor(1.7202)\n",
      "tensor(2.0007)\n",
      "tensor(1.3454)\n",
      "tensor(1.4884)\n",
      "tensor(1.7403)\n",
      "tensor(1.6525)\n",
      "tensor(1.6363)\n",
      "tensor(2.1261)\n",
      "tensor(1.9124)\n",
      "tensor(1.3192)\n",
      "tensor(2.4085)\n",
      "tensor(1.5243)\n",
      "tensor(0.8617)\n",
      "tensor(0.3639)\n",
      "tensor(2.4354)\n",
      "tensor(1.9306)\n",
      "tensor(1.9334)\n",
      "tensor(1.7742)\n",
      "tensor(1.7207)\n",
      "tensor(2.4643)\n",
      "tensor(1.4623)\n",
      "tensor(1.3417)\n",
      "tensor(0.5993)\n",
      "tensor(1.5971)\n",
      "tensor(1.5723)\n",
      "tensor(1.7607)\n",
      "tensor(1.8909)\n",
      "tensor(2.3796)\n",
      "tensor(1.2802)\n",
      "tensor(1.8956)\n",
      "tensor(2.0010)\n",
      "tensor(1.7218)\n",
      "tensor(1.9374)\n",
      "tensor(1.5989)\n",
      "tensor(2.0819)\n",
      "tensor(1.7490)\n",
      "tensor(1.3974)\n",
      "tensor(1.7148)\n",
      "tensor(0.5087)\n",
      "tensor(0.2728)\n",
      "tensor(2.2095)\n",
      "tensor(1.2578)\n",
      "tensor(1.6436)\n",
      "tensor(1.1622)\n",
      "tensor(1.8135)\n",
      "tensor(2.0389)\n",
      "tensor(2.1192)\n",
      "tensor(0.1848)\n",
      "tensor(1.5558)\n",
      "tensor(1.2457)\n",
      "tensor(0.2595)\n",
      "tensor(1.8327)\n",
      "tensor(0.4062)\n",
      "tensor(0.6446)\n",
      "tensor(1.5213)\n",
      "tensor(1.8687)\n",
      "tensor(0.8103)\n",
      "tensor(1.7672)\n",
      "tensor(0.5750)\n",
      "tensor(1.7192)\n",
      "tensor(1.4338)\n",
      "tensor(1.7534)\n",
      "tensor(1.3336)\n",
      "tensor(1.4392)\n",
      "tensor(2.0130)\n",
      "tensor(1.4936)\n",
      "tensor(2.3611)\n",
      "tensor(1.8330)\n",
      "tensor(1.6333)\n",
      "tensor(1.7924)\n",
      "tensor(1.3708)\n",
      "tensor(1.8009)\n",
      "tensor(1.5455)\n",
      "tensor(1.2423)\n",
      "tensor(1.4313)\n",
      "tensor(1.5836)\n",
      "tensor(1.5551)\n",
      "tensor(1.5524)\n",
      "tensor(0.7557)\n",
      "tensor(1.9047)\n",
      "tensor(1.1434)\n",
      "tensor(0.9986)\n",
      "tensor(1.0669)\n",
      "tensor(1.2481)\n",
      "tensor(1.2697)\n",
      "tensor(1.8593)\n",
      "tensor(2.3203)\n",
      "tensor(2.1448)\n",
      "tensor(2.2434)\n",
      "tensor(1.9986)\n",
      "tensor(2.9888)\n",
      "tensor(1.8218)\n",
      "tensor(1.3829)\n",
      "tensor(2.1162)\n",
      "tensor(2.1955)\n",
      "tensor(1.5270)\n",
      "tensor(1.9611)\n",
      "tensor(0.6216)\n",
      "tensor(1.7546)\n",
      "tensor(1.2347)\n",
      "tensor(1.5175)\n",
      "tensor(2.0359)\n",
      "tensor(0.2200)\n",
      "tensor(1.6105)\n",
      "tensor(1.8553)\n",
      "tensor(2.0831)\n",
      "tensor(1.9943)\n",
      "tensor(1.4659)\n",
      "tensor(1.4369)\n",
      "tensor(1.0184)\n",
      "tensor(1.2188)\n",
      "tensor(1.7325)\n",
      "tensor(2.0388)\n",
      "tensor(1.8162)\n",
      "tensor(1.5103)\n",
      "tensor(1.7729)\n",
      "tensor(2.2962)\n",
      "tensor(1.6312)\n",
      "tensor(0.6341)\n",
      "tensor(0.1903)\n",
      "tensor(1.6606)\n",
      "tensor(0.7442)\n",
      "tensor(0.6298)\n",
      "tensor(1.2212)\n",
      "tensor(1.9699)\n",
      "tensor(1.8626)\n",
      "tensor(1.1894)\n",
      "tensor(2.0061)\n",
      "tensor(1.4120)\n",
      "tensor(1.4072)\n",
      "tensor(1.5942)\n",
      "tensor(1.5118)\n",
      "tensor(0.1885)\n",
      "tensor(1.7395)\n",
      "tensor(2.3568)\n",
      "tensor(1.2870)\n",
      "tensor(1.4151)\n",
      "tensor(1.6692)\n",
      "tensor(1.5223)\n",
      "tensor(1.6821)\n",
      "tensor(1.1989)\n",
      "tensor(2.0131)\n",
      "tensor(1.5207)\n",
      "tensor(0.6848)\n",
      "tensor(1.3917)\n",
      "tensor(1.2465)\n",
      "tensor(1.4681)\n",
      "tensor(1.9193)\n",
      "tensor(1.2489)\n",
      "tensor(0.3396)\n",
      "tensor(1.7130)\n",
      "tensor(1.3322)\n",
      "tensor(1.6269)\n",
      "tensor(1.4754)\n",
      "tensor(0.3129)\n",
      "tensor(1.4108)\n",
      "tensor(2.3098)\n",
      "tensor(1.4128)\n",
      "tensor(0.9214)\n",
      "tensor(0.7817)\n",
      "tensor(1.5918)\n",
      "tensor(1.2551)\n",
      "tensor(1.0680)\n",
      "tensor(1.6326)\n",
      "tensor(1.4626)\n",
      "tensor(1.2662)\n",
      "tensor(0.5746)\n",
      "tensor(0.1508)\n",
      "tensor(1.5311)\n",
      "tensor(1.6072)\n",
      "tensor(1.3094)\n",
      "tensor(1.3455)\n",
      "tensor(1.0617)\n",
      "tensor(1.9984)\n",
      "tensor(1.4155)\n",
      "tensor(1.8147)\n",
      "tensor(1.5183)\n",
      "tensor(1.7148)\n",
      "tensor(1.1097)\n",
      "tensor(1.7326)\n",
      "tensor(1.2602)\n",
      "tensor(1.2227)\n",
      "tensor(1.2981)\n",
      "tensor(1.6489)\n",
      "tensor(1.5216)\n",
      "tensor(1.8884)\n",
      "tensor(1.0948)\n",
      "tensor(2.1867)\n",
      "tensor(1.7902)\n",
      "tensor(1.6180)\n",
      "tensor(1.4614)\n",
      "tensor(1.3951)\n",
      "tensor(1.4093)\n",
      "tensor(2.1913)\n",
      "tensor(1.2557)\n",
      "tensor(2.2492)\n",
      "tensor(1.4038)\n",
      "tensor(1.3560)\n",
      "tensor(1.5041)\n",
      "tensor(1.2461)\n",
      "tensor(1.3935)\n",
      "tensor(0.7934)\n",
      "tensor(1.5198)\n",
      "tensor(1.2938)\n",
      "tensor(1.5920)\n",
      "tensor(2.2886)\n",
      "tensor(1.3937)\n",
      "tensor(1.4841)\n",
      "tensor(1.6642)\n",
      "tensor(0.3192)\n",
      "tensor(0.3410)\n",
      "tensor(2.0794)\n",
      "tensor(1.0324)\n",
      "tensor(1.3546)\n",
      "tensor(1.3931)\n",
      "tensor(1.5563)\n",
      "tensor(1.9772)\n",
      "tensor(1.9009)\n",
      "tensor(1.4686)\n",
      "tensor(1.0159)\n",
      "tensor(1.9232)\n",
      "tensor(1.9499)\n",
      "tensor(0.7273)\n",
      "tensor(1.4303)\n",
      "tensor(1.8379)\n",
      "tensor(1.6164)\n",
      "tensor(1.6820)\n",
      "tensor(1.4824)\n",
      "tensor(1.0956)\n",
      "tensor(1.5009)\n",
      "tensor(1.8367)\n",
      "tensor(1.8457)\n",
      "tensor(2.0541)\n",
      "tensor(1.0425)\n",
      "tensor(1.8679)\n",
      "tensor(1.7103)\n",
      "tensor(1.7092)\n",
      "tensor(0.3415)\n",
      "tensor(1.4533)\n",
      "tensor(0.7836)\n",
      "tensor(1.4509)\n",
      "tensor(1.6206)\n",
      "tensor(1.8930)\n",
      "tensor(1.4738)\n",
      "tensor(1.7682)\n",
      "tensor(2.1578)\n",
      "tensor(1.5358)\n",
      "tensor(1.5347)\n",
      "tensor(0.8128)\n",
      "tensor(2.0289)\n",
      "tensor(1.0210)\n",
      "tensor(0.5701)\n",
      "tensor(1.3729)\n",
      "tensor(1.5739)\n",
      "tensor(1.7166)\n",
      "tensor(1.3762)\n",
      "tensor(1.7906)\n",
      "tensor(1.7194)\n",
      "tensor(1.1804)\n",
      "tensor(1.2130)\n",
      "tensor(2.0147)\n",
      "tensor(0.2239)\n",
      "tensor(1.0087)\n",
      "tensor(0.9738)\n",
      "tensor(1.4388)\n",
      "tensor(1.8077)\n",
      "tensor(1.9010)\n",
      "tensor(1.5823)\n",
      "tensor(1.3174)\n",
      "tensor(2.1482)\n",
      "tensor(1.4979)\n",
      "tensor(2.2482)\n",
      "tensor(1.0392)\n",
      "tensor(1.6991)\n",
      "tensor(2.0085)\n",
      "tensor(2.3646)\n",
      "tensor(2.0058)\n",
      "tensor(1.6641)\n",
      "tensor(1.9583)\n",
      "tensor(1.3250)\n",
      "tensor(1.8750)\n",
      "tensor(1.8554)\n",
      "tensor(0.9695)\n",
      "tensor(1.9714)\n",
      "tensor(1.3553)\n",
      "tensor(1.9041)\n",
      "tensor(1.8346)\n",
      "tensor(1.9200)\n",
      "tensor(1.5203)\n",
      "tensor(0.9245)\n",
      "tensor(1.6127)\n",
      "tensor(1.4652)\n",
      "tensor(1.3230)\n",
      "tensor(1.7749)\n",
      "tensor(1.7073)\n",
      "tensor(1.0513)\n",
      "tensor(1.6821)\n",
      "tensor(2.1593)\n",
      "tensor(1.5893)\n",
      "tensor(1.4211)\n",
      "tensor(1.0968)\n",
      "tensor(1.6842)\n",
      "tensor(1.7014)\n",
      "tensor(2.3023)\n",
      "tensor(1.3880)\n",
      "tensor(1.7364)\n",
      "tensor(2.3312)\n",
      "tensor(1.3948)\n",
      "tensor(2.3091)\n",
      "tensor(1.4182)\n",
      "tensor(1.7191)\n",
      "tensor(2.1828)\n",
      "tensor(1.3143)\n",
      "tensor(1.6727)\n",
      "tensor(1.4540)\n",
      "tensor(1.8386)\n",
      "tensor(1.8028)\n",
      "tensor(0.4385)\n",
      "tensor(1.3275)\n",
      "tensor(1.8269)\n",
      "tensor(1.9669)\n",
      "tensor(1.3653)\n",
      "tensor(0.3066)\n",
      "tensor(1.6510)\n",
      "tensor(0.6600)\n",
      "tensor(0.7458)\n",
      "tensor(1.3727)\n",
      "tensor(1.8785)\n",
      "tensor(1.5395)\n",
      "tensor(2.0118)\n",
      "tensor(1.4524)\n",
      "tensor(1.8107)\n",
      "tensor(1.2275)\n",
      "tensor(1.7702)\n",
      "tensor(1.5214)\n",
      "tensor(1.3640)\n",
      "tensor(1.3519)\n",
      "tensor(1.9687)\n",
      "tensor(0.6756)\n",
      "tensor(1.7916)\n",
      "tensor(2.0796)\n",
      "tensor(1.9216)\n",
      "tensor(1.5919)\n",
      "tensor(1.1213)\n",
      "tensor(1.6729)\n",
      "tensor(1.7785)\n",
      "tensor(1.1640)\n",
      "tensor(2.2937)\n",
      "tensor(1.7057)\n",
      "tensor(1.4195)\n",
      "tensor(1.8614)\n",
      "tensor(1.4702)\n",
      "tensor(1.6949)\n",
      "tensor(1.2923)\n",
      "tensor(1.4488)\n",
      "tensor(1.7084)\n",
      "tensor(1.3398)\n",
      "tensor(1.2113)\n",
      "tensor(1.6965)\n",
      "tensor(1.7526)\n",
      "tensor(1.5684)\n",
      "tensor(1.3837)\n",
      "tensor(2.3498)\n",
      "tensor(1.7646)\n",
      "tensor(1.8814)\n",
      "tensor(2.0954)\n",
      "tensor(1.3377)\n",
      "tensor(0.9971)\n",
      "tensor(1.7205)\n",
      "tensor(1.9174)\n",
      "tensor(2.0385)\n",
      "tensor(1.4944)\n",
      "tensor(1.4502)\n",
      "tensor(1.5471)\n",
      "tensor(1.3320)\n",
      "tensor(1.7980)\n",
      "tensor(0.9661)\n",
      "tensor(2.4351)\n",
      "tensor(2.2375)\n",
      "tensor(1.4434)\n",
      "tensor(1.6564)\n",
      "tensor(1.5444)\n",
      "tensor(1.4204)\n",
      "tensor(1.1998)\n",
      "tensor(1.9042)\n",
      "tensor(1.2494)\n",
      "tensor(0.5948)\n",
      "tensor(2.5527)\n",
      "tensor(1.5427)\n",
      "tensor(1.4309)\n",
      "tensor(1.8955)\n",
      "tensor(1.4769)\n",
      "tensor(1.6174)\n",
      "tensor(1.6059)\n",
      "tensor(1.2374)\n",
      "tensor(0.2086)\n",
      "tensor(1.5472)\n",
      "tensor(1.3756)\n",
      "tensor(1.6040)\n",
      "tensor(1.9253)\n",
      "tensor(1.7134)\n",
      "tensor(1.3193)\n",
      "tensor(1.0638)\n",
      "tensor(1.8592)\n",
      "tensor(1.5943)\n",
      "tensor(1.8899)\n",
      "tensor(1.9081)\n",
      "tensor(1.4538)\n",
      "tensor(1.7640)\n",
      "tensor(1.1684)\n",
      "tensor(1.5039)\n",
      "tensor(0.7027)\n",
      "tensor(1.7225)\n",
      "tensor(1.8273)\n",
      "tensor(1.8127)\n",
      "tensor(1.9355)\n",
      "tensor(2.7689)\n",
      "tensor(1.6153)\n",
      "tensor(2.0869)\n",
      "tensor(1.6261)\n",
      "tensor(1.1882)\n",
      "tensor(1.4324)\n",
      "tensor(1.1898)\n",
      "tensor(1.3705)\n",
      "tensor(1.6845)\n",
      "tensor(2.1392)\n",
      "tensor(1.5395)\n",
      "tensor(1.2023)\n",
      "tensor(0.9451)\n",
      "tensor(1.8164)\n",
      "tensor(0.7804)\n",
      "tensor(0.9826)\n",
      "tensor(1.9485)\n",
      "tensor(0.9798)\n",
      "tensor(1.5866)\n",
      "tensor(1.3333)\n",
      "tensor(1.6202)\n",
      "tensor(1.1031)\n",
      "tensor(2.2772)\n",
      "tensor(1.8542)\n",
      "tensor(1.6296)\n",
      "tensor(1.4636)\n",
      "tensor(1.4889)\n",
      "tensor(1.7674)\n",
      "tensor(1.4735)\n",
      "tensor(1.7945)\n",
      "tensor(0.6065)\n",
      "tensor(1.8236)\n",
      "tensor(1.9959)\n",
      "tensor(1.7760)\n",
      "tensor(1.6254)\n",
      "tensor(1.3323)\n",
      "tensor(1.7970)\n",
      "tensor(1.7318)\n",
      "tensor(1.6912)\n",
      "tensor(1.5732)\n",
      "tensor(1.4844)\n",
      "tensor(1.1458)\n",
      "tensor(0.1696)\n",
      "tensor(1.8423)\n",
      "tensor(1.7886)\n",
      "tensor(1.0388)\n",
      "tensor(1.1979)\n",
      "tensor(2.0808)\n",
      "tensor(1.8064)\n",
      "tensor(1.2070)\n",
      "tensor(1.2883)\n",
      "tensor(1.3265)\n",
      "tensor(2.9384)\n",
      "tensor(1.1995)\n",
      "tensor(1.4888)\n",
      "tensor(0.6479)\n",
      "tensor(1.8499)\n",
      "tensor(2.0898)\n",
      "tensor(1.3678)\n",
      "tensor(0.8337)\n",
      "tensor(1.9191)\n",
      "tensor(1.0907)\n",
      "tensor(2.0363)\n",
      "tensor(1.1182)\n",
      "tensor(1.6432)\n",
      "tensor(1.9773)\n",
      "tensor(1.8593)\n",
      "tensor(1.2190)\n",
      "tensor(1.3260)\n",
      "tensor(1.5925)\n",
      "tensor(0.9185)\n",
      "tensor(1.8447)\n",
      "tensor(1.8460)\n",
      "tensor(1.7658)\n",
      "tensor(1.1712)\n",
      "tensor(1.0254)\n",
      "tensor(1.7070)\n",
      "tensor(1.8915)\n",
      "tensor(1.7483)\n",
      "tensor(1.4931)\n",
      "tensor(1.8661)\n",
      "tensor(1.6329)\n",
      "tensor(1.8614)\n",
      "tensor(2.1139)\n",
      "tensor(1.4238)\n",
      "tensor(1.8864)\n",
      "tensor(1.8625)\n",
      "tensor(1.5997)\n",
      "tensor(1.3514)\n",
      "tensor(1.8759)\n",
      "tensor(1.9301)\n",
      "tensor(1.1596)\n",
      "tensor(1.5362)\n",
      "tensor(1.7526)\n",
      "tensor(1.5148)\n",
      "tensor(1.6341)\n",
      "tensor(2.1094)\n",
      "tensor(1.6202)\n",
      "tensor(1.9138)\n",
      "tensor(1.9951)\n",
      "tensor(1.3914)\n",
      "tensor(1.9291)\n",
      "tensor(1.7100)\n",
      "tensor(1.0444)\n",
      "tensor(1.6398)\n",
      "tensor(2.0294)\n",
      "tensor(1.8596)\n",
      "tensor(2.0155)\n",
      "tensor(1.5767)\n",
      "tensor(1.3287)\n",
      "tensor(1.5132)\n",
      "tensor(1.5255)\n",
      "tensor(1.6438)\n",
      "tensor(1.6292)\n",
      "tensor(1.8029)\n",
      "tensor(1.1307)\n",
      "tensor(1.0170)\n",
      "tensor(1.3889)\n",
      "tensor(1.7267)\n",
      "tensor(1.8626)\n",
      "tensor(1.6500)\n",
      "tensor(1.8683)\n",
      "tensor(0.3225)\n",
      "tensor(1.5973)\n",
      "tensor(1.2785)\n",
      "tensor(1.3866)\n",
      "tensor(1.7641)\n",
      "tensor(1.1029)\n",
      "tensor(1.8001)\n",
      "tensor(1.9241)\n",
      "tensor(1.9071)\n",
      "tensor(1.6810)\n",
      "tensor(2.0176)\n",
      "tensor(1.9155)\n",
      "tensor(1.4474)\n",
      "tensor(2.5471)\n",
      "tensor(1.7609)\n",
      "tensor(1.4601)\n",
      "tensor(1.4331)\n",
      "tensor(2.2214)\n",
      "tensor(2.0937)\n",
      "tensor(1.1798)\n",
      "tensor(1.9270)\n",
      "tensor(1.9008)\n",
      "tensor(1.8886)\n",
      "tensor(0.2550)\n",
      "tensor(1.1378)\n",
      "tensor(1.6130)\n",
      "tensor(1.7138)\n",
      "tensor(1.4057)\n",
      "tensor(1.2309)\n",
      "tensor(1.9424)\n",
      "tensor(1.8367)\n",
      "tensor(1.3790)\n",
      "tensor(1.3214)\n",
      "tensor(2.1454)\n",
      "tensor(2.0539)\n",
      "tensor(2.1348)\n",
      "tensor(2.9169)\n",
      "tensor(1.9783)\n",
      "tensor(1.5196)\n",
      "tensor(1.4031)\n",
      "tensor(1.6538)\n",
      "tensor(1.6800)\n",
      "tensor(1.7211)\n",
      "tensor(1.1718)\n",
      "tensor(1.3169)\n",
      "tensor(1.3440)\n",
      "tensor(1.6832)\n",
      "tensor(2.4710)\n",
      "tensor(1.8750)\n",
      "tensor(1.0751)\n",
      "tensor(0.9754)\n",
      "tensor(2.1665)\n",
      "tensor(1.9708)\n",
      "tensor(1.5776)\n",
      "tensor(1.8531)\n",
      "tensor(1.2207)\n",
      "tensor(1.8218)\n",
      "tensor(1.4990)\n",
      "tensor(1.4776)\n",
      "tensor(2.1358)\n",
      "tensor(0.5659)\n",
      "tensor(1.8405)\n",
      "tensor(1.5526)\n",
      "tensor(1.5721)\n",
      "tensor(1.4378)\n",
      "tensor(1.7402)\n",
      "tensor(1.7578)\n",
      "tensor(1.7590)\n",
      "tensor(1.4567)\n",
      "tensor(2.0248)\n",
      "tensor(1.6265)\n",
      "tensor(2.2744)\n",
      "tensor(1.0633)\n",
      "tensor(1.6366)\n",
      "tensor(1.4421)\n",
      "tensor(1.9128)\n",
      "tensor(1.2014)\n",
      "tensor(1.3672)\n",
      "tensor(1.5268)\n",
      "tensor(0.3063)\n",
      "tensor(1.1896)\n",
      "tensor(1.5079)\n",
      "tensor(2.0704)\n",
      "tensor(1.4242)\n",
      "tensor(2.0496)\n",
      "tensor(1.1821)\n",
      "tensor(1.5193)\n",
      "tensor(0.7063)\n",
      "tensor(1.8219)\n",
      "tensor(1.7894)\n",
      "tensor(2.1521)\n",
      "tensor(1.8673)\n",
      "tensor(1.8421)\n",
      "tensor(1.8038)\n",
      "tensor(1.4311)\n",
      "tensor(1.7269)\n",
      "tensor(1.6018)\n",
      "tensor(1.5444)\n",
      "tensor(1.8325)\n",
      "tensor(2.0446)\n",
      "tensor(1.9149)\n",
      "tensor(1.5243)\n",
      "tensor(1.8757)\n",
      "tensor(1.5576)\n",
      "tensor(1.2042)\n",
      "tensor(1.9879)\n",
      "tensor(1.4513)\n",
      "tensor(1.1828)\n",
      "tensor(1.4794)\n",
      "tensor(1.2882)\n",
      "tensor(0.1059)\n",
      "tensor(1.7993)\n",
      "tensor(2.1640)\n",
      "tensor(1.3753)\n",
      "tensor(0.5755)\n",
      "tensor(1.9744)\n",
      "tensor(1.2281)\n",
      "tensor(1.9507)\n",
      "tensor(1.9699)\n",
      "tensor(1.8042)\n",
      "tensor(1.6205)\n",
      "tensor(1.6337)\n",
      "tensor(1.7714)\n",
      "tensor(1.7369)\n",
      "tensor(1.0509)\n",
      "tensor(0.5145)\n",
      "tensor(0.9649)\n",
      "tensor(0.7537)\n",
      "tensor(1.5522)\n",
      "tensor(1.9389)\n",
      "tensor(1.3216)\n",
      "tensor(1.6105)\n",
      "tensor(1.5054)\n",
      "tensor(1.6196)\n",
      "tensor(2.4003)\n",
      "tensor(1.6802)\n",
      "tensor(1.7908)\n",
      "tensor(1.4033)\n",
      "tensor(1.8174)\n",
      "tensor(1.5181)\n",
      "tensor(1.0164)\n",
      "tensor(1.5005)\n",
      "tensor(1.6194)\n",
      "tensor(1.5433)\n",
      "tensor(1.6001)\n",
      "tensor(2.0463)\n",
      "tensor(1.8882)\n",
      "tensor(1.6453)\n",
      "tensor(1.3397)\n",
      "tensor(0.9712)\n",
      "tensor(2.0065)\n",
      "tensor(1.2682)\n",
      "tensor(1.3769)\n",
      "tensor(1.8924)\n",
      "tensor(1.0454)\n",
      "tensor(1.4850)\n",
      "tensor(1.6342)\n",
      "tensor(1.4863)\n",
      "tensor(1.4346)\n",
      "tensor(1.2499)\n",
      "tensor(1.1963)\n",
      "tensor(1.9258)\n",
      "tensor(1.4938)\n",
      "tensor(1.6824)\n",
      "tensor(1.8406)\n",
      "tensor(1.2399)\n",
      "tensor(1.5613)\n",
      "tensor(2.1762)\n",
      "tensor(1.8441)\n",
      "tensor(1.2991)\n",
      "tensor(1.6547)\n",
      "tensor(1.4577)\n",
      "tensor(1.6520)\n",
      "tensor(1.9836)\n",
      "tensor(1.2711)\n",
      "tensor(2.1311)\n",
      "tensor(0.4034)\n",
      "tensor(1.2052)\n",
      "tensor(0.7016)\n",
      "tensor(1.8373)\n",
      "tensor(0.9511)\n",
      "tensor(0.7536)\n",
      "tensor(0.3780)\n",
      "tensor(2.2469)\n",
      "tensor(1.4541)\n",
      "tensor(1.2383)\n",
      "tensor(0.2868)\n",
      "tensor(1.6675)\n",
      "tensor(1.7126)\n",
      "tensor(1.6956)\n",
      "tensor(1.4504)\n",
      "tensor(1.3930)\n",
      "tensor(1.0392)\n",
      "tensor(1.6302)\n",
      "tensor(0.5522)\n",
      "tensor(1.7461)\n",
      "tensor(1.7312)\n",
      "tensor(1.4749)\n",
      "tensor(2.0198)\n",
      "tensor(1.9860)\n",
      "tensor(1.3423)\n",
      "tensor(1.5987)\n",
      "tensor(1.9460)\n",
      "tensor(2.5815)\n",
      "tensor(1.3199)\n",
      "tensor(2.1415)\n",
      "tensor(1.3350)\n",
      "tensor(0.5863)\n",
      "tensor(1.1173)\n",
      "tensor(1.5376)\n",
      "tensor(0.7902)\n",
      "tensor(2.2034)\n",
      "tensor(1.0120)\n",
      "tensor(1.4142)\n",
      "tensor(1.7479)\n",
      "tensor(1.5089)\n",
      "tensor(1.0435)\n",
      "tensor(2.1109)\n",
      "tensor(1.2769)\n",
      "tensor(0.7991)\n",
      "tensor(1.1133)\n",
      "tensor(1.5320)\n",
      "tensor(2.0624)\n",
      "tensor(1.8668)\n",
      "tensor(1.2677)\n",
      "tensor(1.8591)\n",
      "tensor(1.2481)\n",
      "tensor(1.4386)\n",
      "tensor(1.2170)\n",
      "tensor(1.4876)\n",
      "tensor(1.5007)\n",
      "tensor(1.2620)\n",
      "tensor(1.1612)\n",
      "tensor(1.1579)\n",
      "tensor(1.6390)\n",
      "tensor(1.4478)\n",
      "tensor(2.6985)\n",
      "tensor(1.3630)\n",
      "tensor(1.3865)\n",
      "tensor(0.7740)\n",
      "tensor(1.0355)\n",
      "tensor(1.2876)\n",
      "tensor(1.5858)\n",
      "tensor(1.4824)\n",
      "tensor(1.7399)\n",
      "tensor(1.9356)\n",
      "tensor(1.6928)\n",
      "tensor(1.6358)\n",
      "tensor(1.1160)\n",
      "tensor(1.8382)\n",
      "tensor(1.1795)\n",
      "tensor(1.2475)\n",
      "tensor(0.9527)\n",
      "tensor(0.3729)\n",
      "tensor(1.7953)\n",
      "tensor(1.2238)\n",
      "tensor(1.4288)\n",
      "tensor(1.5329)\n",
      "tensor(2.2278)\n",
      "tensor(1.6721)\n",
      "tensor(2.6832)\n",
      "tensor(1.5309)\n",
      "tensor(1.6298)\n",
      "tensor(0.2787)\n",
      "tensor(1.6046)\n",
      "tensor(1.6139)\n",
      "tensor(1.5843)\n",
      "tensor(2.0497)\n",
      "tensor(1.7244)\n",
      "tensor(1.8920)\n",
      "tensor(1.2495)\n",
      "tensor(1.7797)\n",
      "tensor(1.4677)\n",
      "tensor(1.8864)\n",
      "tensor(1.3401)\n",
      "tensor(1.1581)\n",
      "tensor(2.0804)\n",
      "tensor(1.6121)\n",
      "tensor(1.6460)\n",
      "tensor(1.4491)\n",
      "tensor(1.5055)\n",
      "tensor(1.1372)\n",
      "tensor(1.9076)\n",
      "tensor(1.2777)\n",
      "tensor(1.6109)\n",
      "tensor(1.2299)\n",
      "tensor(1.3735)\n",
      "tensor(0.0200)\n"
     ]
    }
   ],
   "source": [
    "gradsimilarsum = 0\n",
    "for i in range(1000):\n",
    "    # Get a batch of images and labels\n",
    "    dataiter = iter(trainloader)\n",
    "    images, labels = next(dataiter)\n",
    "\n",
    "    # Select the first two images\n",
    "    images = images[:2]\n",
    "\n",
    "    # Make sure the images require gradient computation\n",
    "    images.requires_grad = True\n",
    "\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.grad = None\n",
    "    # Forward pass\n",
    "    outputs = model(images.view(images.shape[0], -1))\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = criterion(outputs, labels[:2])\n",
    "\n",
    "\n",
    "    # Backward pass from the loss\n",
    "    loss.backward()\n",
    "\n",
    "    # The gradient of the loss with respect to the images is now stored in `images.grad`\n",
    "    for i, param in enumerate(model.parameters()):\n",
    "    if i != 0:  # Skip the first weight matrix\n",
    "        param.grad = None\n",
    "\n",
    "    # The gradient of the loss with respect to the first weight matrix is now stored in `model[0].weight.grad`\n",
    "    print(model[0].weight.grad)\n",
    "\n",
    "\n",
    "    grad1 = images.grad[0].flatten()\n",
    "    grad2 = images.grad[1].flatten()\n",
    "\n",
    "    # Calculate the cosine of the angle between the gradients\n",
    "    cos_theta = torch.dot(grad1, grad2) / (grad1.norm() * grad2.norm())\n",
    "\n",
    "    import numpy as np\n",
    "    print(np.arccos(cos_theta))\n",
    "    gradsimilarsum += cos_theta\n",
    "\n",
    "print(gradsimilarsum/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the MNIST images: 98 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in trainloader:\n",
    "        images, labels = data\n",
    "        images = images.view(images.shape[0], -1)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the MNIST images: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0141,  0.0249, -0.0261,  0.0234,  0.0770, -0.0089, -0.0094,  0.0564,\n",
      "         0.0429,  0.0763, -0.0822, -0.0057,  0.0786, -0.0046,  0.0505,  0.0685,\n",
      "         0.0302,  0.0342,  0.0795, -0.0053, -0.0778,  0.0535,  0.0232,  0.0393,\n",
      "        -0.0400,  0.0345, -0.0084, -0.0701,  0.0253,  0.0590,  0.0279,  0.0558,\n",
      "        -0.0234,  0.0850,  0.0549, -0.0092, -0.0049,  0.0250, -0.0293,  0.0196,\n",
      "         0.0806, -0.0774, -0.0842,  0.0095, -0.0533, -0.0578,  0.0043, -0.0841,\n",
      "        -0.0788, -0.0138, -0.0849,  0.0452, -0.0492,  0.0366,  0.0328,  0.0515,\n",
      "        -0.0652,  0.0434,  0.0676, -0.0823,  0.0244, -0.0605, -0.0237,  0.0662,\n",
      "         0.0005, -0.0786,  0.0071,  0.0836, -0.0132,  0.0642, -0.0510,  0.0389,\n",
      "         0.0196, -0.0815,  0.0061,  0.0611, -0.0264, -0.0248,  0.0173,  0.0132,\n",
      "         0.0792,  0.0265,  0.0733,  0.0241,  0.0293,  0.0329,  0.0823, -0.0612,\n",
      "        -0.0086, -0.0329,  0.0488,  0.0639,  0.0656,  0.0185,  0.0784,  0.0575,\n",
      "        -0.0719, -0.0827,  0.0218, -0.0588,  0.0070, -0.0779,  0.0282,  0.0243,\n",
      "        -0.0821, -0.0852,  0.0754,  0.0445, -0.0679, -0.0514,  0.0493, -0.0810,\n",
      "         0.0486,  0.0302,  0.0223, -0.0434,  0.0188, -0.0119,  0.0683, -0.0174,\n",
      "         0.0403,  0.0632, -0.0233, -0.0796,  0.0752,  0.0239,  0.0126, -0.0593],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model[4].weight[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_accuracy(model):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        k = 0\n",
    "        for data in trainloader:\n",
    "            k += 1\n",
    "            if k > 5:\n",
    "                break\n",
    "            images, labels = data\n",
    "            images = images.view(images.shape[0], -1)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the MNIST images: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_biases(input_model, layer, alpha):\n",
    "    bias_guesses = []\n",
    "    losses = []\n",
    "    bias_guesses.append(0 * torch.randn(input_model[layer].bias.shape))\n",
    "    for i in range(49):\n",
    "        # create random matrix of the same size as the layer\n",
    "        bias_guesses.append(alpha * torch.randn(input_model[layer].bias.shape))\n",
    "    for i in range(50):\n",
    "        loss_sum = 0\n",
    "        # add the matrix to the model\n",
    "        input_model[layer].bias.data.add_(bias_guesses[i])\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        k = 0\n",
    "        for j in range(2):\n",
    "            data = next(iter(trainloader))\n",
    "            images, labels = data\n",
    "            images = images.view(images.shape[0], -1)\n",
    "            outputs = input_model(images)\n",
    "            loss_sum += criterion(outputs, labels).item()\n",
    "        losses.append(loss_sum / 2)  # Append the average loss\n",
    "        input_model[layer].bias.data.add_(-bias_guesses[i])\n",
    "\n",
    "\n",
    "    minindex = losses.index(min(losses))\n",
    "    input_model[layer].bias.data.add_(bias_guesses[minindex])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the MNIST images: 9 %\n",
      "Accuracy of the network on the MNIST images: 8 %\n",
      "Accuracy of the network on the MNIST images: 19 %\n",
      "Accuracy of the network on the MNIST images: 21 %\n",
      "Accuracy of the network on the MNIST images: 21 %\n",
      "Accuracy of the network on the MNIST images: 18 %\n",
      "Accuracy of the network on the MNIST images: 21 %\n",
      "Accuracy of the network on the MNIST images: 20 %\n",
      "Accuracy of the network on the MNIST images: 21 %\n",
      "Accuracy of the network on the MNIST images: 25 %\n",
      "Accuracy of the network on the MNIST images: 21 %\n",
      "Accuracy of the network on the MNIST images: 21 %\n",
      "Accuracy of the network on the MNIST images: 21 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 75\u001b[0m\n\u001b[0;32m     73\u001b[0m update_row(model2, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m.4\u001b[39m\u001b[38;5;241m/\u001b[39m((i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m.2\u001b[39m))\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# update_biases(model2, 0, .4/((i+1)**.2))\u001b[39;00m\n\u001b[1;32m---> 75\u001b[0m \u001b[43mupdate_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.4\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m.2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# update_biases(model2, 2, .4/((i+1)**.2))\u001b[39;00m\n\u001b[0;32m     77\u001b[0m update_row(model2, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m.4\u001b[39m\u001b[38;5;241m/\u001b[39m((i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m.2\u001b[39m))\n",
      "Cell \u001b[1;32mIn[14], line 54\u001b[0m, in \u001b[0;36mupdate_row\u001b[1;34m(input_model, layer, alpha)\u001b[0m\n\u001b[0;32m     52\u001b[0m loss_sum \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m---> 54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m     56\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mview(images\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Bela\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Bela\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Bela\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Bela\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Bela\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\datasets\\mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    142\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 145\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32mc:\\Users\\Bela\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\Bela\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Bela\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\transforms\\functional.py:170\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pic\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    169\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;241m*\u001b[39m img\n\u001b[1;32m--> 170\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_image_num_channels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# put it from HWC to CHW format\u001b[39;00m\n\u001b[0;32m    172\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#model with absolutely random weight perturbations\n",
    "\n",
    "model2 = nn.Sequential(nn.Linear(784, 128), nn.ReLU(), nn.Linear(128, 128), nn.ReLU(), nn.Linear(128, 10), nn.LogSoftmax(dim=1))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# so basically weights[0] is 784 x 128 matrix, weights[2] is 128 x 128 matrix, weights[4] is 128 x 10 matrix\n",
    "\n",
    "# make a 100 random 784 x 128 matrices\n",
    "\n",
    "# try the model with each of these random matrices\n",
    "\n",
    "# na jo az nagyon lassu lesz, de legalabb mukodik\n",
    "\n",
    "# to do: configure learning rate\n",
    "\n",
    "\n",
    "\n",
    "def update_row(input_model, layer, alpha):\n",
    "    losses = []\n",
    "    rand_weights = []\n",
    "    rand_weights.append(0 * torch.randn(input_model[layer].weight.shape))\n",
    "    for i in range(49):\n",
    "        # create random matrix of the same size as the layer\n",
    "        rand_weights.append(alpha * torch.randn(input_model[layer].weight.shape))\n",
    "    for i in range(50):\n",
    "        # add the matrix to the model\n",
    "\n",
    "        # images, labels = next(iter(trainloader))\n",
    "        # images = images.view(images.shape[0], -1)\n",
    "\n",
    "        # output = input_model(images[0].view(1, -1))\n",
    "\n",
    "        # criterion = nn.NLLLoss()\n",
    "        # loss = criterion(output, labels[0].view(1))\n",
    "        # loss.backward()\n",
    "        # gradient = input_model[layer].weight.grad\n",
    "        \n",
    "\n",
    "        # #compute cosine similarity\n",
    "        # grad1 = gradient.flatten()\n",
    "        # grad2 = rand_weights[i].flatten()\n",
    "\n",
    "        # # Calculate the cosine of the angle between the gradients\n",
    "        # cos_theta = torch.dot(grad1, grad2) / (grad1.norm() * grad2.norm())\n",
    "        # print(cos_theta)\n",
    "        # print(grad2.norm())\n",
    "\n",
    "\n",
    "\n",
    "        input_model[layer].weight.data.add_(rand_weights[i])\n",
    "        # on a mini batch of 64 images, calculate the loss\n",
    "        loss_sum = 0\n",
    "        for j in range(2):\n",
    "            data = next(iter(trainloader))\n",
    "            images, labels = data\n",
    "            images = images.view(images.shape[0], -1)\n",
    "            outputs = input_model(images)\n",
    "            loss_sum += criterion(outputs, labels).item()\n",
    "        losses.append(loss_sum / 2)  # Append the average loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        input_model[layer].weight.data.add_(-rand_weights[i])\n",
    "\n",
    "    #choose the best one and add it to the network\n",
    "    best = losses.index(min(losses))\n",
    "    input_model[layer].weight.data.add_(rand_weights[best])\n",
    "\n",
    "for i in range(100000):\n",
    "    update_row(model2, 0, .4/((i+1)**.2))\n",
    "    # update_biases(model2, 0, .4/((i+1)**.2))\n",
    "    update_row(model2, 2, .4/((i+1)**.2))\n",
    "    # update_biases(model2, 2, .4/((i+1)**.2))\n",
    "    update_row(model2, 4, .4/((i+1)**.2))\n",
    "    # update_biases(model2, 4, .4/((i+1)**.2))\n",
    "    if i % 10 == 0:\n",
    "        print_accuracy(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 2060 with Max-Q Design\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity with random guessing\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'trainloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 93\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28mprint\u001b[39m(avg_cos_normal\u001b[38;5;241m/\u001b[39msample_size)\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28mprint\u001b[39m(avg_cos_wt\u001b[38;5;241m/\u001b[39msample_size)\n\u001b[1;32m---> 93\u001b[0m \u001b[43mcompare_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n",
      "Cell \u001b[1;32mIn[1], line 12\u001b[0m, in \u001b[0;36mcompare_gradients\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcosine similarity with random guessing\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(sample_size):\n\u001b[1;32m---> 12\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(\u001b[43mtrainloader\u001b[49m))\n\u001b[0;32m     13\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mview(images\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     15\u001b[0m     images\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trainloader' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import ipdb\n",
    "\n",
    "\n",
    "def compare_gradients():\n",
    "\n",
    "    sample_size = 10\n",
    "\n",
    "    avg_cos_normal = 0\n",
    "    print(\"cosine similarity with random guessing\")\n",
    "    for i in range(sample_size):\n",
    "        images, labels = next(iter(trainloader))\n",
    "        images = images.view(images.shape[0], -1)\n",
    "\n",
    "        images.requires_grad = True\n",
    "        model.zero_grad()\n",
    "        outputs = model(images)\n",
    "        criterion = nn.NLLLoss()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        grad1 = model[0].weight.grad.flatten()\n",
    "        print(grad1.shape)\n",
    "\n",
    "\n",
    "        grad2 = torch.randn(model[0].weight.shape).flatten()\n",
    "        cos_theta = torch.dot(grad1, grad2) / (grad1.norm() * grad2.norm())\n",
    "        avg_cos_normal += abs(cos_theta)\n",
    "\n",
    "\n",
    "    #cosine similarity with W_t\n",
    "    avg_cos_wt = 0    \n",
    "    for i in range(sample_size):\n",
    "        layer = 2\n",
    "        images, labels = next(iter(trainloader))\n",
    "        images = images.view(images.shape[0], -1)\n",
    "        images.requires_grad = True\n",
    "\n",
    "\n",
    "\n",
    "        model.zero_grad()\n",
    "        outputs = model(images)\n",
    "        criterion = nn.NLLLoss()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        grad1 = model[layer].weight.grad.flatten()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        def get_input(layer):\n",
    "            def hook(module, input, output):\n",
    "                # input is a tuple, to get the actual input tensor, use input[0]\n",
    "                layer.input = input[0]\n",
    "            return hook\n",
    "\n",
    "        # Attach the hook to the 2nd layer (index 1 if 0-based indexing)\n",
    "        hook = model[layer].register_forward_hook(get_input(model[layer]))\n",
    "\n",
    "        output = model(images)\n",
    "\n",
    "        # Now you can access both the input and output of the layer\n",
    "        x_i = model[layer].input\n",
    "        w_i = model[layer].weight\n",
    "        w_iplus1 = model[layer+2].weight\n",
    "        x_i_matrix = x_i[0].view(x_i[0].shape[0],-1)\n",
    "        s_i = torch.mm(w_i, x_i_matrix)\n",
    "\n",
    "\n",
    "\n",
    "        deltaReLU = torch.diag((s_i.flatten() > 0).float())\n",
    "        \n",
    "\n",
    "        intermed1 = torch.mm(torch.randn(1, model[layer+2].out_features), w_iplus1)\n",
    "        intermed2 = torch.mm(intermed1, deltaReLU)\n",
    "\n",
    "        grad2 = torch.mm(intermed2.t(), x_i_matrix.t())\n",
    "\n",
    "\n",
    "        grad2 = grad2.flatten()\n",
    "\n",
    "        \n",
    "\n",
    "        cos_theta = torch.dot(grad1, grad2) / (grad1.norm() * grad2.norm())\n",
    "\n",
    "        avg_cos_wt += abs(cos_theta)\n",
    "\n",
    "\n",
    "    print(avg_cos_normal/sample_size)\n",
    "    print(avg_cos_wt/sample_size)\n",
    "\n",
    "compare_gradients() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Accuracy of the network on the MNIST images: 12 %\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "Accuracy of the network on the MNIST images: 11 %\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "Accuracy of the network on the MNIST images: 13 %\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "Accuracy of the network on the MNIST images: 6 %\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "Accuracy of the network on the MNIST images: 14 %\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "Accuracy of the network on the MNIST images: 9 %\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "Accuracy of the network on the MNIST images: 8 %\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[81], line 113\u001b[0m\n\u001b[0;32m    111\u001b[0m update_weights_smart(model3, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m.4\u001b[39m\u001b[38;5;241m/\u001b[39m((i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m.2\u001b[39m))\n\u001b[0;32m    112\u001b[0m \u001b[38;5;66;03m#update_biases(model3, 2, .1/((i+1)**.2))\u001b[39;00m\n\u001b[1;32m--> 113\u001b[0m \u001b[43mupdate_weights_smart\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.4\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m.2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m#update_biases(model3, 4, .1/((i+1)**.2))\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28mprint\u001b[39m(i)\n",
      "Cell \u001b[1;32mIn[81], line 23\u001b[0m, in \u001b[0;36mupdate_weights_smart\u001b[1;34m(input_model, layer, alpha)\u001b[0m\n\u001b[0;32m     19\u001b[0m     gradient_guesses\u001b[38;5;241m.\u001b[39mappend(alpha \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, si_size))\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m50\u001b[39m):\n\u001b[1;32m---> 23\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mview(images\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# image = images[0]\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m# image = image.view(1, -1)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Bela\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Bela\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Bela\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Bela\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Bela\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\datasets\\mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    142\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 145\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32mc:\\Users\\Bela\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\Bela\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Bela\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\transforms\\transforms.py:277\u001b[0m, in \u001b[0;36mNormalize.forward\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    270\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;124;03m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Bela\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\transforms\\functional.py:363\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m    361\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg should be Tensor Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensor)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Bela\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\transforms\\_functional_tensor.py:922\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    920\u001b[0m mean \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(mean, dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mtensor\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    921\u001b[0m std \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(std, dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mtensor\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 922\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43m(\u001b[49m\u001b[43mstd\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43many\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    923\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstd evaluated to zero after conversion to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, leading to division by zero.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    924\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mean\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "model3 = nn.Sequential(nn.Linear(784, 128), nn.ReLU(), nn.Linear(128, 128), nn.ReLU(), nn.Linear(128, 10), nn.LogSoftmax(dim=1))\n",
    "\n",
    "\n",
    "def update_weights_smart(input_model, layer, alpha):\n",
    "    si_size = 0\n",
    "    if layer == 0:\n",
    "        si_size = 128\n",
    "    elif layer == 2:\n",
    "        si_size = 10\n",
    "    elif layer == 4:\n",
    "        si_size = 10\n",
    "    losses = []\n",
    "    gradient_guesses = []\n",
    "    gradient_guesses.append(0 * torch.randn(1, si_size))\n",
    "    weight_perturbation_list = []\n",
    "    # avg_cos_similarity = 0\n",
    "    for i in range(49):\n",
    "        # create random matrix of the same size as the layer\n",
    "        gradient_guesses.append(alpha * torch.randn(1, si_size))\n",
    "    for i in range(50):\n",
    "\n",
    "\n",
    "        images, labels = next(iter(trainloader))\n",
    "        images = images.view(images.shape[0], -1)\n",
    "        # image = images[0]\n",
    "        # image = image.view(1, -1)\n",
    "\n",
    "        \n",
    "        def get_input(layer):\n",
    "            def hook(module, input, output):\n",
    "                # input is a tuple, to get the actual input tensor, use input[0]\n",
    "                layer.input = input[0]\n",
    "                # layer.input = sum(input)/len(input)\n",
    "            return hook\n",
    "\n",
    "        # Attach the hook to the 2nd layer (index 1 if 0-based indexing)\n",
    "        hook = input_model[layer].register_forward_hook(get_input(input_model[layer]))\n",
    "\n",
    "        output = input_model(images)\n",
    "\n",
    "\n",
    "        # # # compute the actual gradient\n",
    "        # criterion = nn.NLLLoss()\n",
    "        # loss = criterion(output, labels)\n",
    "        # loss.backward()\n",
    "        # gradient = input_model[layer].weight.grad\n",
    "\n",
    "\n",
    "        # Now you can access both the input and output of the layer\n",
    "        if layer == 0 or layer == 2:\n",
    "            x_i = input_model[layer].input\n",
    "            w_i = input_model[layer].weight\n",
    "            w_iplus1 = input_model[layer+2].weight\n",
    "            x_i_matrix = x_i[0].view(x_i[0].shape[0],-1)\n",
    "            bias_vector = input_model[layer].bias\n",
    "            s_i = torch.mm(w_i, x_i_matrix)\n",
    "            bias = bias_vector.view(s_i.shape)\n",
    "            s_i = s_i + bias\n",
    "\n",
    "            deltaReLU = torch.diag((s_i.flatten() > 0).float())\n",
    "            intermed1 = torch.mm(gradient_guesses[i], w_iplus1)\n",
    "            intermed2 = torch.mm(intermed1, deltaReLU)\n",
    "            weight_perturbation = torch.mm(intermed2.t(), x_i_matrix.t())\n",
    "\n",
    "        if layer == 4:\n",
    "            x_i = input_model[layer].input\n",
    "            w_i = input_model[layer].weight\n",
    "            x_i_matrix = x_i[0].view(x_i[0].shape[0],-1)\n",
    "            bias_vector = input_model[layer].bias\n",
    "            s_i = torch.mm(w_i, x_i_matrix)\n",
    "            bias = bias_vector.view(s_i.shape)\n",
    "            s_i = s_i + bias\n",
    "            deltaReLU = torch.diag((s_i.flatten() > 0).float())\n",
    "            intermed1 = torch.mm(gradient_guesses[i], deltaReLU)\n",
    "            weight_perturbation = torch.mm(intermed1.t(), x_i_matrix.t())\n",
    "\n",
    "        weight_perturbation_list.append(weight_perturbation)\n",
    "\n",
    "        # compute cosine similarity \n",
    "        # grad1 = gradient.flatten()\n",
    "        # grad2 = weight_perturbation.flatten()\n",
    "        # cos_theta = torch.dot(grad1, grad2) / (grad1.norm() * grad2.norm())\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        input_model[layer].weight.data.add_(weight_perturbation)\n",
    "        loss_sum = 0\n",
    "        for j in range(2):\n",
    "            data = next(iter(trainloader))\n",
    "            images, labels = data\n",
    "            images = images.view(images.shape[0], -1)\n",
    "            outputs = input_model(images)\n",
    "            loss_sum += criterion(outputs, labels).item()\n",
    "        losses.append(loss_sum/2)  # Append the average loss\n",
    "        input_model[layer].weight.data.add_(-weight_perturbation)\n",
    "\n",
    "        hook.remove()\n",
    "    #choose the best one and add it to the network\n",
    "        \n",
    "    best = losses.index(min(losses))\n",
    "    input_model[layer].weight.data.add_(weight_perturbation_list[best])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(2000):\n",
    "    update_weights_smart(model3, 0, .4/((i+1)**.2))\n",
    "    #update_biases(model3, 0, .1/((i+1)**.2))\n",
    "    update_weights_smart(model3, 2, .4/((i+1)**.2))\n",
    "    #update_biases(model3, 2, .1/((i+1)**.2))\n",
    "    update_weights_smart(model3, 4, .4/((i+1)**.2))\n",
    "    #update_biases(model3, 4, .1/((i+1)**.2))\n",
    "    print(i)\n",
    "    if i % 10 == 0:\n",
    "        print_accuracy(model3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(640, 10)\n",
      "[8.7495718e+00 8.4884481e+00 8.0213404e+00 7.9393139e+00 7.9024944e+00\n",
      " 7.7789826e+00 7.6357222e+00 7.0931702e+00 6.3491158e+00 7.1567416e-07]\n",
      "Effective rank: 10\n"
     ]
    }
   ],
   "source": [
    "# check bias cosine similarity\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_bias_gradients_matrix(input_model):\n",
    "    # Ensure model is in training mode\n",
    "    input_model.train()\n",
    "\n",
    "    # Initialize a matrix to store the gradients\n",
    "    gradients_matrix = []\n",
    "\n",
    "    # Get a batch of 64 images\n",
    "    for i in range(10):\n",
    "        images, labels = next(iter(trainloader))\n",
    "        images = images.view(images.shape[0], -1)\n",
    "\n",
    "        for i in range(images.shape[0]):\n",
    "            # Zero the gradients\n",
    "            input_model.zero_grad()\n",
    "\n",
    "            # Forward pass for one image at a time\n",
    "            output = input_model(images[i].unsqueeze(0))\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(output, labels[i].unsqueeze(0))\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Get the gradients of the biases of the first layer and add them to the matrix\n",
    "            gradients_matrix.append(input_model[4].bias.grad.view(-1).detach().numpy())\n",
    "\n",
    "        # input_model.zero_grad()\n",
    "        # outputs = input_model(images)\n",
    "        # criterion = nn.NLLLoss()\n",
    "        # loss = criterion(outputs, labels)\n",
    "        # loss.backward()\n",
    "        # gradients_matrix.append(input_model[2].bias.grad.view(-1).detach().numpy())\n",
    "\n",
    "\n",
    "    return np.array(gradients_matrix)\n",
    "\n",
    "\n",
    "\n",
    "def effective_rank(matrix, tolerance=1e-8):\n",
    "    # print size of array\n",
    "    print(matrix.shape)\n",
    "\n",
    "\n",
    "    # Compute SVD\n",
    "    u, s, vh = np.linalg.svd(matrix)\n",
    "\n",
    "    print(s)\n",
    "\n",
    "    # Count singular values that are not close to zero\n",
    "    rank = np.sum(s > tolerance)\n",
    "\n",
    "    return rank\n",
    "\n",
    "# Usage:\n",
    "matrix = get_bias_gradients_matrix(model3)\n",
    "rank = effective_rank(matrix)\n",
    "print('Effective rank:', rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Digit = 3\n",
      "Actual Digit = 3\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have a DataLoader `trainloader` and a trained model `model`\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "# Take the first image from the batch\n",
    "img = images[0]\n",
    "\n",
    "# Convert the image data into a format that can be input into the model\n",
    "img = img.resize_(1, 784)\n",
    "\n",
    "# Make a forward pass through the model and get the predictions\n",
    "with torch.no_grad():\n",
    "    logps = model(img)\n",
    "\n",
    "# Convert the log probabilities to probabilities\n",
    "ps = torch.exp(logps)\n",
    "probab = list(ps.numpy()[0])\n",
    "\n",
    "# Print the image and the model's prediction\n",
    "print(\"Predicted Digit =\", probab.index(max(probab)))\n",
    "print(\"Actual Digit =\", labels[0].item())\n",
    "plt.imshow(images[0].numpy().squeeze(), cmap='gray_r');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
